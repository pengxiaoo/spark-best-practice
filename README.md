# best practice with Spark Core and Spark SQL

## 用DataFrame替代RDD
RDD是Spark Core的核心抽象，而DataFrame是Spark SQL的核心抽象。从性能角度，DataFrame相比RDD有巨大优势，主要体现在：
>* 支持谓词下推。对logical plan重排序，把过滤操作推到尽可能靠近数据源的地方，从而让下游的操作在尽可能小的数据集上进行。
>* 处理每条数据时，不需要针对完整的scala object，而只需要针对必要的field，从而大幅减少需要反序列化以及网络传输的数据量。
>* 更快、更省内存的序列化和反序列化方式
>* 数据的列式存储。更高的存储效率，更少的IO(why?)
>* 支持off-heap的内存管理，从而使应用程序免受GC造成的停顿和时延

为什么DataFrame会有性能提升？因为相较RDD而言，DataFrame对数据和操作有更深入的洞察：
1. DataFrame支持的数据类型都是预定义好的数据类型，数据以结构化形式存储，DataFrame知道数据的schema，可以对数据的存储和操作进行优化。而RDD中，数据的存储结构是不透明的，对象是不透明的、任意的，无法做假设，也无法做优化。
2. DataFrame支持的操作都是预定义好的高度优化过的关系型操作（如Select, Order by, Filter, Group by等等），而RDD的操作是可任意定义的functional操作。functional操作是不透明的，无法做优化。

DataFrame是如何进行优化的？答案是通过Catalyst和Tungsten：
1. Catalyst Query Optimizer。Catalyst负责把Spark SQL代码编译成RDD代码。在这个过程中，由于Catalyst知道数据的schema，也了解每一种关系型操作，因此可以对计算过程进行优化。例如：对logical plan进行重排序，把filter尽可能往前推；对于每一行数据（即一个serialized scala object），只select, deserialize必要的column（即object的一个field），而不需要deserialize整个object再进行操作；Catalyst还可以跟特定数据源结合进行深度优化，例如如果Spark SQL对MongoDB上建了索引的字段进行query，那么可以利用MongoDB的in-db aggregation，只提取相关记录，避免全表扫描。
2. Tungsten Off-heap Serializer。由于DataFrame的数据类型都是预先限定好的，且数据的schema已知，因此Tungsten可以充分利用这些信息，高效的实现数据的序列化和反序列化。无论是内存占用还是时间消耗，Tungsten的序列化和反序列化都大大优于java原生的Serializer以及Kryo。此外，Tungsten是列式存储的，而数据处理中大部分情况都是基于列进行选择、聚合或排序，所以列式存储的设计可以减少IO、提高处理效率、提高数据压缩效率。最后，Tungsten支持off-heap的方式来管理内存，从而免受GC的影响。

DataFrame并不是免费的，它也有使用上的限制条件和代价：
1. 数据必须要有schema，如果数据本身是非结构化的（例如图片，文本），很难提取schema，那么没法用DataFrame
2. 数据必须表达成Spark SQL预定义的数据类型。否则Tungsten不会工作
3. DataFrame支持的操作不像RDD那么丰富和底层。例如RDD有丰富的API可以方便的控制partitioning，而DataFrame则没有。

## 减少shuffle
shuffle意味着网络传输，而网络传输意味着高时延。通过尽可能的减少shuffle，把时间花在计算而非网络传输，可以提高Spark应用的执行效率和速度。减少shuffle有多种方式：
1. 采用mapper side reduce来减少shuffle。例如用reduceByKey代替groupByKey+reduce，这样在shuffle之前先做一轮reduce，可以大幅减少需要shuffle的数据量；
2. 通过Pre-partition来减少shuffle。例如一种情况，假设需要周期性的对两个RDD进行join，其中一个RDD是静态的、不随时间变化的（例如用户注册信息），另一个RDD是动态的、时变的（例如用户在某个时间段内的活动），那么在join之前先对静态RDD进行pre-partition，这样每次join时，静态RDD的partitioner已知，只有时变RDD会发生shuffle(why?)。
3. 通过broadcast来减少shuffle。例如一大一小两个RDD进行join，那么可以先把小的RDD collect到driver上形成一个查找表，然后把这个查找表作为广播变量传播到各个executor上，然后对大的RDD进行mapPartitions，每个partition跟查找表做local combine。这样可以达到join的效果并完全避免了shuffle。这种join有个专门的名字叫“broadcast hash join”，事实上，Spark SQL支持自动进行broadcast hash join(可参考Dataset.scala的hint方法)，而Spark Core则需要手动去写代码实现

## 正确使用persist（缓存）
Spark提供了两种手段来reuse RDD/Dataframe，分别是persist和checkpoint。

persist（即缓存）是在同一个Spark Job中，把RDD/Dataframe缓存到executor的内存或磁盘中，以实现重用，避免重复计算。假如某一个RDD会在两个不同的action中被用到，那么把这个RDD缓存起来可以避免计算两次（即当第一个action计算的过程中得到了这个RDD的时候，这个RDD会缓存在spark的内存或磁盘上，当第二个action需要用到这个RDD的时候，可以直接读缓存，而不必重新计算）。

checkpoint把RDD/Dataframe写入到Spark之外作为备份（通常写入到S3或HDFS这种可靠存储上，用户自定义的一个checkpoint目录下）， 被checkpoint的数据不受Spark Job生命周期的限制，也不占用Spark的内存资源，但是要付出磁盘存储和IO的代价。如果一个Spark Job的计算比较耗资源（包括较多的内存占用，较长的运行时间），同时集群也比较繁忙，Job有一定概率失败并且一旦失败代价较大，这时把Job的中间结果进行checkpoint有助于降低失败概率和避免重复计算。

初学者常常犯的错误是误解了persist，另外对checkpoint不够了解，在不需要persist的时候却进行了persist，在需要checkpoint的时候却没有checkpoint。他们潜意识里认为persist对fault tolerance有帮助，但其实persist并不是fault tolerance的手段（checkpoint姑且算是），不必要的缓存只是单纯的浪费了内存资源，对计算并没有任何帮助。

## 使用广播变量
广播变量是Spark中的一种数据分发方式，可以高效的把driver中的对象分发到各个executor。假如某个transformation的lambda表达式中用到了一个查找表，如果采用普通的方式，这个查找表会随着lambda表达式复制到每一个相关的task中，如果有1000个task就会复制1000份，这既浪费了driver的传输带宽，也浪费了executor的内存资源；如果采用广播变量的方式，那么查找表会通过高效的传播协议（多点到多点，类似BitTorrent）来传播，并且每个executor只复制一份，同一个executor中的不同tasks共享这一份数据。

## 处理数据倾斜
Spark的job以shuffle为界划分成一个个stage，stage之间串行运行，每个stage都要等前一个stage结束才能开始运行。每个stage包含多个并行处理单元也就是task，不同的task在不同的数据子集上执行相同的处理逻辑，一个stage的运行时间取决于这个stage里运行最久的那个task的运行耗时。理想情况下，同一个stage的不同的task分配到的数据量规模和运行耗时应该大体接近，但实际中往往不是如此。例如有时候一个stage的大部分task都只花了几秒或十几秒，但是有个别task却花了几十分钟，严重拖慢了stage进度，这种情况通常就是发生了数据倾斜。 

数据倾斜也就是数据在不同task之间划分的不均匀。在基于key的aggregation或join操作中，如果某些key出现的次数明显多于别的key，由于同一个key的数据一定会拉取到同一个task中处理，那么这个倒霉的task就需要处理比别的task多得多的数据，从而成为整个stage的瓶颈。

对于aggregation中的数据倾斜，通常的应对思路是把集中的key进行打散，通过二次聚合的方式，把高负荷task的数据分散给更多的task并行处理。以一个实际的场景为例：现在有一个用户购买记录的数据集，假设每条record包含的字段有userId、购买的产品信息、以及支付金额，我们需要统计每个用户的总支付金额。现知数据存在严重的倾斜，表现在大部分userId只有一两条购买记录，但是小部分高频用户有上万条记录。我们可以通过一些手段（数据量小的话可以通过reduceByKey统计每个userId出现的次数，然后sort并取最高频的N个userId，数据量大的话可以先sample）找到这些高频用户，把高频用户单独作为一个RDD特殊处理，普通用户作为另一个RDD按常规方式处理（两个RDD处理结果union起来就是最终结果）；在高频用户RDD中，每个userId加上一个取值为1~n的随机前缀，假设有一个高频userId“Alice”出现了很多次，加上随机前缀之后就会变成”1_Alice”，”2_Alice”，…,“n_Alice”，这样的话原来必须由同一个task聚合的”Alice”就可以分给多个task聚合，最后只需要把n个”*_Alice”二次聚合，就可以得到“Alice”总的聚合结果。

对于join中的数据倾斜，可以采用broadcast hash join的方式来解决。假设要对两个RDD进行join，其中一个RDD（称其为rdd1）里存在着一些高频key，那么首先找到这些高频key，然后对另一个RDD(称其为rdd2）进行filter和collect，把高频key对应的数据以HashMap的形式保存下来，并把这个hashMap作为广播变量传播到每一个executor上。接下来，rdd1通过hashMap进行过滤并分成两部分：一部分是高频key对应的数据，这部分跟hashMap通过local combine生成join结果；另一部分是正常key对应的数据，这部分可以直接跟rdd2进行join。两部分的结果union起来就是最终结果。

## 用foreachPartitions代替foreach(类似的，用mapPartitions代替map)
foreach是对每一个记录进行操作，而foreachPartitions则是对每一个partition进行操作。很多时候采用foreachPartitions会比foreach更加高效。例如Spark往数据库里写数据的时候，如果采用foreach，那么每写一条记录都需要新建一个数据库连接，既没必要又极大的浪费资源；采用foreachPartitions的话可以每个partition建立一个数据库连接，然后每个partition通过批量的方式高效写入。关于利用foreachPartitions写入数据库的优化，Spark官方文档https://spark.apache.org/docs/latest/streaming-programming-guide.html 的"Design Patterns for using foreachRDD"这一段写得非常好。

## 设计高效的数据结构
在设计数据结构时要充分考虑内存使用效率。能用Int当做key就不要用String；如果需要对数值进行排序，能用Int/Long就不要用Double；能用array就不要用collection；能用String就不要用对象，如果非用对象不可，尽量避免多层嵌套；关于这部分可参考spark官方调优文档https://spark.apache.org/docs/latest/tuning.html

## 合适的并行度
合适的并行度对于spark任务的高效运行非常重要。理论上讲如果每个task计算时间相同，每个CPU core分配一个task即可以实现充分的平行，但实际中不同task处理的任务量划分很难完全均匀，可能会有部分task分到的数据较少并很快执行完，所以通常情况下task的个数要大于core的个数，以避免个别core长时间空闲。spark官方推荐task的个数为spark集群core个数的两到三倍。

## 用mapValues代替map（flatMapValues代替flatMap）
mapValues和flatMapValues是Pair RDD独有的算子，他们可以保持原有的key和partition不变，只对value进行操作。在某些场景中，维持原有partition可以方便下游操作并减少shuffle。对于Pair RDD而言，mapValues和flatMapValues通常情况下可以代替map和flatMap。
